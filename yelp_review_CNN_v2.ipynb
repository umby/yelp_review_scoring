{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from IPython.display import clear_output\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = 'data/yelp_academic_dataset_review.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to clean the data\n",
    "def clean_string(string):\n",
    "    string = string.lower()\n",
    "    # remove web addresses\n",
    "    string = re.sub(r\"(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \n",
    "                    \" \", string)\n",
    "    string = re.sub(r\"[^a-z0-9!\\?\\']\", \" \", string)\n",
    "    string = re.sub(r\"'s\", \" 's\", string)\n",
    "    string = re.sub(r\"s' \", \" 's\", string)\n",
    "    string = re.sub(r\"'ve\", \" 've\", string)\n",
    "    string = re.sub(r\"n't\", \" n't\", string)\n",
    "    string = re.sub(r\"'re\", \" 're\", string)\n",
    "    string = re.sub(r\"'d\", \" 'd\", string)\n",
    "    string = re.sub(r\"'ll\", \" 'll\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    #string = re.sub(r\"?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean up the text to only use ascii characters\n",
    "def clean_file(file_path, print_every=100000, write_out=True):\n",
    "    \"\"\"\n",
    "    Read through the CSV file and generate a dictionary of words\n",
    "    with counts and [optional] create a new cleaned file.\n",
    "    \n",
    "    Returns word_count dictionary, and cleaned file path\n",
    "    \"\"\"\n",
    "    word_count = defaultdict(int)\n",
    "    start = time.time()\n",
    "    T0 = time.time()\n",
    "    if write_out:\n",
    "        p, e = os.path.splitext(file_path)\n",
    "        out_path = p + '_clean' + e\n",
    "        #fp_out = open(out_path, 'w', encoding='utf-8')\n",
    "        fp_out = open(out_path, 'w')\n",
    "        fp_out.write('review,stars\\n')\n",
    "\n",
    "    #with open(file_path, 'r', encoding='utf-8') as fp:\n",
    "    with open(file_path, 'r') as fp:\n",
    "        reader = csv.reader(fp)\n",
    "        header = next(reader)\n",
    "        text_index = header.index('text')\n",
    "        star_index = header.index('stars')\n",
    "\n",
    "        for i,line in enumerate(reader):\n",
    "            review_text = clean_string(line[text_index])\n",
    "            stars = line[star_index]\n",
    "            if write_out and len(review_text)>0:\n",
    "                fp_out.write(review_text+','+stars+'\\n')\n",
    "            review = review_text.split(' ')\n",
    "            for word in review:\n",
    "                word_count[word] += 1\n",
    "            if i%print_every == 0:\n",
    "                if write_out:\n",
    "                    fp_out.flush()\n",
    "                print('{} reviews completed, dictionary size: {}, cycle time: {:.1f} seconds.'.format(\n",
    "                    i, len(word_count), time.time()-T0))\n",
    "                T0 = time.time()\n",
    "    print('{} reviews completed.  Dictionary size: {}'.format(i, len(word_count)))\n",
    "    print('Total time: {}'.format(str(timedelta(seconds=round(time.time()-start)))))\n",
    "    if write_out:\n",
    "        fp_out.close()\n",
    "    return word_count, out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wordcount(cleaned_file_path):\n",
    "    \"\"\"\n",
    "    Generates a dictionary of {word, count} from a file that has already\n",
    "    been cleaned.\n",
    "    \"\"\"\n",
    "    longest_review_length = 0\n",
    "    word_count = defaultdict(int)\n",
    "    with open(cleaned_file_path, 'r') as fp:\n",
    "        #header = next(fp)\n",
    "        for rev_count, line in enumerate(fp):\n",
    "            review_text, stars = line.split(',')\n",
    "            review = review_text.split(' ')\n",
    "            if longest_review_length < len(review):\n",
    "                longest_review_length = len(review)\n",
    "            for word in review:\n",
    "                word_count[word] += 1\n",
    "            if rev_count%100000 == 0:\n",
    "                print('Words in {} reviews counted.'.format(rev_count))\n",
    "    return word_count, rev_count, longest_review_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_vocab(word_count, remove_lowest_fraction=0.01):\n",
    "    \"\"\"\n",
    "    Creates a vocabulary by removing the bottom `remove_lowest_fraction` \n",
    "    of the words passed in via `word_count`.\n",
    "    : inputs : \n",
    "    word_count: a diction of {word, int} pairs\n",
    "    remove_lowest_fraction: the loweset occuring fraction out of 1.0 to\n",
    "        drop from the vocab\n",
    "\n",
    "    : output :\n",
    "    vocab : a dictionary of {word, index} to to use for mapping words to ints\n",
    "    \"\"\"\n",
    "    total_words = sum(list(word_count.values()))\n",
    "    threshold = 0\n",
    "    sum_below_threshold = 0\n",
    "    while float(sum_below_threshold) / total_words < remove_lowest_fraction:\n",
    "        threshold += 1\n",
    "        sum_below_threshold = sum([v for v in word_count.values() if v<=threshold])\n",
    "\n",
    "    print('Word frequency threshold = {} at {:.3f}% percent of total words.'.format(threshold,\n",
    "        100*sum_below_threshold/total_words))\n",
    "    print('Vocab size at 100% = {}, vocab size at {:.3f}% = {}'.format(\n",
    "        len(word_count), \n",
    "        (100-100.0*sum_below_threshold/total_words),\n",
    "        len([w for w,v in word_count.items() if v > threshold])))\n",
    "\n",
    "    ## create a list of all word above that threshold\n",
    "    ## create a dictionary that maps all valid words to a number\n",
    "    vocab = sorted(word_count, key=word_count.get, reverse=True)\n",
    "    # include '.' which is not in word_count so that 0 is open to use a filler\n",
    "    vocab = dict((v,i) for i,v in enumerate(chain('.', vocab)) \n",
    "                if word_count[v]>threshold)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_nonvocab_words(cleaned_file_path, vocab):\n",
    "    \"\"\"\n",
    "    Reads through cleaned file and remove words not in `vocab` and \n",
    "    removes reviews with zero remaining words.\n",
    "\n",
    "    Writes a new file to ``CLEAN_FILENAME``+``_vocab_only``.csv\n",
    "\n",
    "    Returns a tuple the number of words in the longest review, the\n",
    "    number of reviews, and cleaned and vocab-only file path.\n",
    "    \"\"\"\n",
    "    p, e = os.path.splitext(file_path)\n",
    "    out_path = p + '_vocab_only' + e\n",
    "    longest_review_length = 0\n",
    "    number_reviews = 0\n",
    "    #with open(cleaned_file_path,'r') as fp, open(out_path, 'w', encoding='utf-8') as fp_out:\n",
    "    with open(cleaned_file_path,'r') as fp, open(out_path, 'w') as fp_out:\n",
    "        fp_out.write(next(fp))\n",
    "        for i, line in enumerate(fp):\n",
    "            review_text, stars = line.split(',')\n",
    "            review = [w for w in review_text.split(' ') if w in vocab]\n",
    "            if longest_review_length < len(review):\n",
    "                longest_review_length = len(review)\n",
    "            if len(review) > 0:\n",
    "                fp_out.write(' '.join(review)+','+stars)\n",
    "                number_reviews += 1\n",
    "            if i%100000 == 0:\n",
    "                print('{} reviews cleaned and removed non-vocab words.'.format(i))\n",
    "    return longest_review_length, number_reviews, out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_to_arrays(file_path, vocab, max_review_length, number_reviews):\n",
    "    \"\"\"\n",
    "    Warning: this will need about 5 GB of RAM.\n",
    "\n",
    "    : inputs :\n",
    "    file_path: [string] the path to cleaned csv with non-vocab words already \n",
    "               removed\n",
    "    vocab: [dictionary] mapping of {word, int} from words to integers\n",
    "    max_review_length: [int] the number of words in the longest review \n",
    "    number_reviews: [int] total number of reviews\n",
    "\n",
    "    : outputs :\n",
    "    r_arr: [np.array] a 2D integer array representation of the reviews. The\n",
    "           space for extra words are padded with zeros\n",
    "    s_arr: [np.array] a 2D integer array of the star ratings.\n",
    "    \"\"\"\n",
    "    r_arr = np.zeros((number_reviews, max_review_length), dtype=np.int16)\n",
    "    s_arr = np.zeros((number_reviews, 5), dtype=np.int8)\n",
    "    with open(file_path, 'r') as fp:\n",
    "        header = next(fp)\n",
    "        for i, line in enumerate(fp):\n",
    "            review, stars = line.split(',')\n",
    "            rev = [vocab[w] for w in review.split(' ')] \n",
    "            stars = int(stars)\n",
    "            r_arr[i,:len(rev)] = rev\n",
    "            s_arr[i,stars-1] = 1\n",
    "    return r_arr, s_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 reviews completed, dictionary size: 62, cycle time: 0.0 seconds.\n",
      "100000 reviews completed, dictionary size: 80002, cycle time: 21.4 seconds.\n",
      "200000 reviews completed, dictionary size: 109027, cycle time: 20.2 seconds.\n",
      "300000 reviews completed, dictionary size: 135672, cycle time: 21.0 seconds.\n",
      "400000 reviews completed, dictionary size: 158039, cycle time: 21.0 seconds.\n",
      "500000 reviews completed, dictionary size: 181024, cycle time: 22.3 seconds.\n",
      "600000 reviews completed, dictionary size: 201110, cycle time: 23.5 seconds.\n",
      "700000 reviews completed, dictionary size: 219979, cycle time: 24.0 seconds.\n",
      "800000 reviews completed, dictionary size: 236165, cycle time: 22.5 seconds.\n",
      "900000 reviews completed, dictionary size: 250889, cycle time: 20.8 seconds.\n",
      "1000000 reviews completed, dictionary size: 264761, cycle time: 21.7 seconds.\n",
      "1100000 reviews completed, dictionary size: 278684, cycle time: 22.5 seconds.\n",
      "1200000 reviews completed, dictionary size: 306638, cycle time: 21.4 seconds.\n",
      "1300000 reviews completed, dictionary size: 324468, cycle time: 23.0 seconds.\n",
      "1400000 reviews completed, dictionary size: 336858, cycle time: 21.8 seconds.\n",
      "1500000 reviews completed, dictionary size: 348341, cycle time: 21.1 seconds.\n",
      "1600000 reviews completed, dictionary size: 361417, cycle time: 22.3 seconds.\n",
      "1700000 reviews completed, dictionary size: 386685, cycle time: 21.7 seconds.\n",
      "1800000 reviews completed, dictionary size: 398146, cycle time: 21.9 seconds.\n",
      "1900000 reviews completed, dictionary size: 409200, cycle time: 21.0 seconds.\n",
      "2000000 reviews completed, dictionary size: 420015, cycle time: 21.1 seconds.\n",
      "2100000 reviews completed, dictionary size: 430238, cycle time: 21.3 seconds.\n",
      "2200000 reviews completed, dictionary size: 440012, cycle time: 19.9 seconds.\n",
      "2300000 reviews completed, dictionary size: 452644, cycle time: 19.8 seconds.\n",
      "2400000 reviews completed, dictionary size: 461110, cycle time: 19.9 seconds.\n",
      "2500000 reviews completed, dictionary size: 469683, cycle time: 19.7 seconds.\n",
      "2600000 reviews completed, dictionary size: 477992, cycle time: 19.1 seconds.\n",
      "2685065 reviews completed.  Dictionary size: 484991\n",
      "Total time: 0:09:32\n"
     ]
    }
   ],
   "source": [
    "word_count, cleaned_file_path = clean_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequency threshold = 137 at 1.000% percent of total words.\n",
      "Vocab size at 100% = 484991, vocab size at -100.059% = 25551\n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 reviews cleaned and removed non-vocab words.\n",
      "100000 reviews cleaned and removed non-vocab words.\n",
      "200000 reviews cleaned and removed non-vocab words.\n",
      "300000 reviews cleaned and removed non-vocab words.\n",
      "400000 reviews cleaned and removed non-vocab words.\n",
      "500000 reviews cleaned and removed non-vocab words.\n",
      "600000 reviews cleaned and removed non-vocab words.\n",
      "700000 reviews cleaned and removed non-vocab words.\n",
      "800000 reviews cleaned and removed non-vocab words.\n",
      "900000 reviews cleaned and removed non-vocab words.\n",
      "1000000 reviews cleaned and removed non-vocab words.\n",
      "1100000 reviews cleaned and removed non-vocab words.\n",
      "1200000 reviews cleaned and removed non-vocab words.\n",
      "1300000 reviews cleaned and removed non-vocab words.\n",
      "1400000 reviews cleaned and removed non-vocab words.\n",
      "1500000 reviews cleaned and removed non-vocab words.\n",
      "1600000 reviews cleaned and removed non-vocab words.\n",
      "1700000 reviews cleaned and removed non-vocab words.\n",
      "1800000 reviews cleaned and removed non-vocab words.\n",
      "1900000 reviews cleaned and removed non-vocab words.\n",
      "2000000 reviews cleaned and removed non-vocab words.\n",
      "2100000 reviews cleaned and removed non-vocab words.\n",
      "2200000 reviews cleaned and removed non-vocab words.\n",
      "2300000 reviews cleaned and removed non-vocab words.\n",
      "2400000 reviews cleaned and removed non-vocab words.\n",
      "2500000 reviews cleaned and removed non-vocab words.\n",
      "2600000 reviews cleaned and removed non-vocab words.\n"
     ]
    }
   ],
   "source": [
    "longest_rev_length, number_reviews, vocab_only_fp = remove_nonvocab_words(cleaned_file_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094 2684822 data/yelp_academic_dataset_review_vocab_only.csv\n"
     ]
    }
   ],
   "source": [
    "print longest_rev_length, number_reviews, vocab_only_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in 0 reviews counted.\n",
      "Words in 100000 reviews counted.\n",
      "Words in 200000 reviews counted.\n",
      "Words in 300000 reviews counted.\n",
      "Words in 400000 reviews counted.\n",
      "Words in 500000 reviews counted.\n",
      "Words in 600000 reviews counted.\n",
      "Words in 700000 reviews counted.\n",
      "Words in 800000 reviews counted.\n",
      "Words in 900000 reviews counted.\n",
      "Words in 1000000 reviews counted.\n",
      "Words in 1100000 reviews counted.\n",
      "Words in 1200000 reviews counted.\n",
      "Words in 1300000 reviews counted.\n",
      "Words in 1400000 reviews counted.\n",
      "Words in 1500000 reviews counted.\n",
      "Words in 1600000 reviews counted.\n",
      "Words in 1700000 reviews counted.\n",
      "Words in 1800000 reviews counted.\n",
      "Words in 1900000 reviews counted.\n",
      "Words in 2000000 reviews counted.\n",
      "Words in 2100000 reviews counted.\n",
      "Words in 2200000 reviews counted.\n",
      "Words in 2300000 reviews counted.\n",
      "Words in 2400000 reviews counted.\n",
      "Words in 2500000 reviews counted.\n",
      "Words in 2600000 reviews counted.\n"
     ]
    }
   ],
   "source": [
    "# When the cleaned file is ready\n",
    "#vocab, number_reviews,longest_rev_length = get_wordcount(vocab_only_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_mapped, stars_wide = text_to_arrays(vocab_only_fp, vocab, longest_rev_length, number_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the pre-processed dataset\n",
    "np.save('cleaned_vocab.npy', vocab)\n",
    "np.save('reviews_mapped.npy', reviews_mapped)\n",
    "np.save('stars_wide.npy', stars_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the pre-processed dataset\n",
    "vocab = np.load('cleaned_vocab.npy').item()\n",
    "reviews_mapped = np.load('reviews_mapped.npy')\n",
    "stars_wide = np.load('stars_wide.npy')\n",
    "longest_rev_length = 1094\n",
    "number_reviews = 2684822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create training dataset and test dataset\n",
    "review_count = len(reviews_mapped)\n",
    "#test_data = reviews_mapped[:int(review_count/3)]\n",
    "#test_labels = stars_wide[:int(review_count/3)]\n",
    "train_data = reviews_mapped[int(review_count/3):]\n",
    "train_labels = stars_wide[int(review_count/3):]\n",
    "review_mapped = None\n",
    "stars_wide = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mini training dataset contains 1700388 samples.\n",
      "The dev dataset contains 89494 samples.\n",
      "[ 791   13 2367 ...,    0    0    0] [0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Split training dataset to dev dataset and mini-train dataset\n",
    "# the dev % is changed for memory issues\n",
    "train_count = len(train_data)\n",
    "dev_data = train_data[:int(train_count/20)]\n",
    "dev_labels = train_labels[:int(train_count/20)]\n",
    "mini_train_data = train_data[int(train_count/20):]\n",
    "mini_train_labels = train_labels[int(train_count/20):]\n",
    "train_data = None\n",
    "train_labels = None\n",
    "\n",
    "print 'The mini training dataset contains %d samples.' %(len(mini_train_labels))\n",
    "print 'The dev dataset contains %d samples.' %(len(dev_labels)) \n",
    "\n",
    "print mini_train_data[0], mini_train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \n",
    "    Baseline model based off of example from WildML (link below)\n",
    "    http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates batches of the data so everything is not stored in memory at once\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-13T00:40:23.482611: step 501, loss 1.3798, acc 0.399\n",
      "2016-12-13T00:40:58.576990: step 502, loss 1.26706, acc 0.479\n",
      "2016-12-13T00:41:33.895657: step 503, loss 1.24166, acc 0.503\n",
      "2016-12-13T00:42:09.242689: step 504, loss 1.37156, acc 0.404\n",
      "2016-12-13T00:42:44.878295: step 505, loss 1.23941, acc 0.508\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4910769e7f86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mevaluate_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-4910769e7f86>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x_batch, y_batch)\u001b[0m\n\u001b[0;32m     79\u001b[0m             _, step, summaries, loss, accuracy = sess.run(\n\u001b[0;32m     80\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 feed_dict)\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 382\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    383\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 655\u001b[1;33m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 723\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    724\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    728\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#sets values for the default graph\n",
    "#defined here to enable easier updating/modifications\n",
    "dropout_keep_prob = 0.5\n",
    "batch_size = 1000  #64, 8192\n",
    "num_epochs = 1 #200\n",
    "evaluate_every = 10\n",
    "checkpoint_every = 200\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(longest_rev_length)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=longest_rev_length,\n",
    "            num_classes=5,\n",
    "            vocab_size=len(vocab)+1,\n",
    "            embedding_size=128,\n",
    "            filter_sizes=[3,4,5],\n",
    "            num_filters=3)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        #curretnly using the mini set and dev set\n",
    "        batches = batch_iter(list(zip(mini_train_data, mini_train_labels)), batch_size, num_epochs)\n",
    "        #dev_batches = batch_iter(list(zip(dev_data, dev_labels)), 200, 1)\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                clear_output()\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_batches = batch_iter(list(zip(dev_data, dev_labels)), batch_size, 1)\n",
    "                for dev_batch in dev_batches:\n",
    "                    dev_x_batch, dev_y_batch = zip(*dev_batch)\n",
    "                    dev_step(dev_x_batch, dev_y_batch, writer=dev_summary_writer)\n",
    "                #dev_step(dev_data, dev_labels, writer=dev_summary_writer)\n",
    "                clear_output()\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint_dir=\"runs/1481516781/checkpoints/\"\n",
    "checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "batch_size = 1000\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"The name 'input_x' refers to an Operation not in the graph.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-4ff55f31ba4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Get the placeholders from the graph by name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0minput_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_operation_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"input_x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.outputs[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m# input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_operation_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2508\u001b[0m       raise TypeError(\"Operation names are strings (or similar), not %s.\"\n\u001b[0;32m   2509\u001b[0m                       % type(name).__name__)\n\u001b[1;32m-> 2510\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2512\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2383\u001b[0m     \"\"\"\n\u001b[0;32m   2384\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2385\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2387\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2444\u001b[0m           raise KeyError(\"The name %s refers to an Operation not in the \"\n\u001b[1;32m-> 2445\u001b[1;33m                          \"graph.\" % repr(name))\n\u001b[0m\u001b[0;32m   2446\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The name 'input_x' refers to an Operation not in the graph.\""
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "              \n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "        \n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        test_batches = batch_iter(list(test_data), batch_size, 1)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in [0,1,2]:\n",
    "        #for x_test_batch in test_batches:\n",
    "\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "\n",
    "correct_predictions = float(sum(all_predictions == test_labels))\n",
    "print(\"Total number of test examples: {}\".format(len(test_labels)))\n",
    "print(\"Accuracy: {:g}\".format(correct_predictions/float(len(test_labels))))\n",
    "\n",
    "# Save the evaluation to a csv\n",
    "#predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions))\n",
    "predictions_human_readable = np.column_stack((test_labels, all_predictions))\n",
    "out_path = os.path.join(checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(predictions_human_readable)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from IPython.display import clear_output\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "csv_file = 'data/yelp_academic_dataset_review.csv'\n",
    "review_data = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 2685066 reviews.\n",
      "The dataset columns:\n",
      "Index([u'user_id', u'review_id', u'text', u'votes.cool', u'business_id',\n",
      "       u'votes.funny', u'stars', u'date', u'type', u'votes.useful'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "review_count = len(review_data.index)\n",
    "print 'The dataset contains %d reviews.' % review_count\n",
    "print 'The dataset columns:'\n",
    "print review_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset contains 1700541 review.\n",
      "The test dataset contains 895022 review.\n",
      "The dev dataset contains 89502 review.\n",
      "It's pretty simple. You take the test online, you pass, then you show up here and give them $15 and they take your pic and give you your TAM card. \n",
      "The online class is about 30-40 minutes. I guess it depends on how fast your read and your reading comprehension. You can also show up here and take the class as well. I recommend doing it online first. 3\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset and test dataset\n",
    "\n",
    "reviews = review_data['text'].tolist()\n",
    "stars = review_data['stars'].tolist()\n",
    "review_count = len(stars)\n",
    "test_text, test_labels = reviews[:review_count/3], stars[:review_count/3]\n",
    "train_text, train_labels = reviews[review_count/3:review_count*29/30], stars[review_count/3:review_count*29/30]\n",
    "dev_text, dev_labels = reviews[:review_count/30], stars[:review_count/30]\n",
    "print 'The training dataset contains %d review.' %(len(train_text))\n",
    "print 'The test dataset contains %d review.' %(len(test_text))\n",
    "print 'The dev dataset contains %d review.' %(len(dev_text))\n",
    "\n",
    "print train_text[1], train_labels[1]\n",
    "\n",
    "# Clear memory\n",
    "review_data = None\n",
    "reviews = None\n",
    "stars = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_process(rawtext):\n",
    "\n",
    "    # Remove special characters. Change to lower case.\n",
    "    text = [re.sub(r\"[^a-z0-9!\\?\\']\", \" \", s.lower()) for s in rawtext] \n",
    "\n",
    "    text = [re.sub(r\"'s\", \" 's\", s) for s in text]\n",
    "    text = [re.sub(r\"'ve\", \" 've\", s) for s in text]\n",
    "    text = [re.sub(r\"n't\", \" n't\", s) for s in text]\n",
    "    text = [re.sub(r\"'re\", \" 're\", s) for s in text]\n",
    "    text = [re.sub(r\"!\", \" ! \", s) for s in text]\n",
    "    text = [re.sub(r\"\\?\", \" \\? \", s) for s in text]\n",
    "    #text = [re.sub(r\"\\s{2,}\", \" \", s) for s in text]\n",
    "    # Stemming the words\n",
    "    #text = [re.sub(r'ing\\b|tion\\b|ly\\b|ed\\b|ious\\b|ies\\b|ive\\b|es\\b|s\\b|ment\\b|ingly\\b|tions\\b|ful\\b|fully\\b','', s) for s in text]        \n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 's pretty simple  you take the test online  you pass  then you show up here and give them  15 and they take your pic and give you your tam card   the online class is about 30 40 minutes  i guess it depends on how fast your read and your reading comprehension  you can also show up here and take the class as well  i recommend doing it online first \n"
     ]
    }
   ],
   "source": [
    "train_text = text_process(train_text)\n",
    "dev_text = text_process(dev_text)\n",
    "#test_text = text_process(test_text)\n",
    "\n",
    "print train_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_process(labels):\n",
    "  label_arr = np.zeros((len(labels), 5), dtype=np.int8)\n",
    "  for i, star in enumerate(labels):\n",
    "    label_arr[i, int(star)-1] = 1\n",
    "  return label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0] [0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "train_labels = label_process(train_labels)\n",
    "dev_labels = label_process(dev_labels)\n",
    "print train_labels[1], dev_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('train_labels.npy', train_labels)\n",
    "np.save('dev_labels.npy', dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1700541x23918 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 71604346 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV = CountVectorizer(stop_words = 'english', min_df = 0.00005)\n",
    "CV.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = CV.vocabulary_.keys()\n",
    "words = [i.encode(\"utf-8\") for i in words]\n",
    "\n",
    "index_vocab = dict(enumerate(words))\n",
    "vocab_index = {v:k for k,v in index_vocab.iteritems()}\n",
    "\n",
    "np.save('index_vocab.npy', index_vocab)\n",
    "np.save('vocab_index.npy', vocab_index)\n",
    "\n",
    "CV = None\n",
    "words = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = [[vocab_index.get(w) for w in r.split(' ')] for r in train_text]\n",
    "train_data = [[w for w in r if w != None] for r in train_data]\n",
    "\n",
    "dev_data = [[vocab_index.get(w) for w in r.split(' ')] for r in dev_text]\n",
    "dev_data = [[w for w in r if w != None] for r in dev_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620\n"
     ]
    }
   ],
   "source": [
    "max_review_length = max(len(max(train_data, key=len)), len(max(dev_data, key=len)))\n",
    "print max_review_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_expand(data, max_review_length):\n",
    "  review_count = len(data)\n",
    "  data_arr = np.zeros((len(data), max_review_length), dtype=np.int16)\n",
    "  for i in range(len(data)):\n",
    "    data_arr[i, :len(data[i])] = data[i]\n",
    "  return data_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19053 10615 18210  2486 14942 22867  8613 11531  1731  2486  5247 16989\n",
      "  3027 13008  7484  3943 10163 12691 16118  2836  5247  9821 14217  2486\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "train_data = data_expand(train_data, max_review_length)\n",
    "dev_data = data_expand(dev_data, max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('train_data.npy', train_data)\n",
    "np.save('dev_data.npy', dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_index = np.load('vocab_index.npy').item()\n",
    "dev_data = np.load('dev_data.npy')\n",
    "train_data = np.load('train_data.npy')\n",
    "dev_labels = np.load('dev_labels.npy')\n",
    "train_labels = np.load('train_labels.npy')\n",
    "max_review_length = 620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \n",
    "    Baseline model based off of example from WildML (link below)\n",
    "    http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates batches of the data so everything is not stored in memory at once\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /home/ubuntu/266-Yelp/runs/1481616593/checkpoints/model-2400\n",
      "\n",
      "2016-12-14T05:12:12.374554: step 2401, loss 0.980585, acc 0.526\n",
      "2016-12-14T05:12:45.827062: step 2402, loss 0.96847, acc 0.566\n",
      "2016-12-14T05:13:18.633000: step 2403, loss 0.988054, acc 0.56\n",
      "2016-12-14T05:13:51.550991: step 2404, loss 0.913415, acc 0.554\n",
      "2016-12-14T05:14:25.798584: step 2405, loss 0.892236, acc 0.594\n",
      "2016-12-14T05:14:59.845407: step 2406, loss 0.886458, acc 0.594\n",
      "2016-12-14T05:15:32.920905: step 2407, loss 0.919317, acc 0.596\n",
      "2016-12-14T05:16:06.573031: step 2408, loss 0.779488, acc 0.72\n",
      "2016-12-14T05:16:40.934638: step 2409, loss 0.789995, acc 0.702\n",
      "2016-12-14T05:17:14.934242: step 2410, loss 0.915428, acc 0.588\n",
      "2016-12-14T05:17:44.359897: step 2411, loss 0.876896, acc 0.604\n",
      "2016-12-14T05:18:16.072451: step 2412, loss 1.01668, acc 0.572\n",
      "2016-12-14T05:18:45.434911: step 2413, loss 0.762405, acc 0.692\n",
      "2016-12-14T05:19:20.014371: step 2414, loss 0.908844, acc 0.628\n",
      "2016-12-14T05:20:00.784881: step 2415, loss 0.839635, acc 0.622\n",
      "2016-12-14T05:20:40.113665: step 2416, loss 0.923379, acc 0.624\n",
      "2016-12-14T05:21:13.253489: step 2417, loss 0.436858, acc 0.856\n",
      "2016-12-14T05:21:45.813188: step 2418, loss 0.225486, acc 0.958\n",
      "2016-12-14T05:22:18.645686: step 2419, loss 0.691046, acc 0.76\n",
      "2016-12-14T05:22:51.338192: step 2420, loss 0.934585, acc 0.572\n",
      "2016-12-14T05:23:24.974780: step 2421, loss 0.797767, acc 0.658\n",
      "2016-12-14T05:23:59.365570: step 2422, loss 0.88134, acc 0.662\n",
      "2016-12-14T05:24:32.144295: step 2423, loss 0.854574, acc 0.636\n",
      "2016-12-14T05:25:06.454156: step 2424, loss 0.903928, acc 0.612\n",
      "2016-12-14T05:25:39.071420: step 2425, loss 0.836133, acc 0.652\n",
      "2016-12-14T05:26:13.347500: step 2426, loss 0.805303, acc 0.624\n",
      "2016-12-14T05:26:47.961263: step 2427, loss 0.906029, acc 0.61\n",
      "2016-12-14T05:27:18.910035: step 2428, loss 0.591385, acc 0.766\n",
      "2016-12-14T05:27:56.887717: step 2429, loss 0.624973, acc 0.766\n",
      "2016-12-14T05:28:41.803603: step 2430, loss 0.939504, acc 0.602\n",
      "2016-12-14T05:29:24.503580: step 2431, loss 0.870201, acc 0.612\n",
      "2016-12-14T05:30:07.620674: step 2432, loss 0.809614, acc 0.644\n",
      "2016-12-14T05:30:52.564301: step 2433, loss 0.870507, acc 0.646\n",
      "2016-12-14T05:31:37.367622: step 2434, loss 0.885575, acc 0.646\n",
      "2016-12-14T05:32:24.758381: step 2435, loss 0.690065, acc 0.724\n",
      "2016-12-14T05:33:15.362110: step 2436, loss 0.842441, acc 0.616\n",
      "2016-12-14T05:34:03.068906: step 2437, loss 0.903191, acc 0.61\n",
      "2016-12-14T05:34:50.374249: step 2438, loss 0.925213, acc 0.602\n",
      "2016-12-14T05:35:37.730833: step 2439, loss 0.581609, acc 0.782\n",
      "2016-12-14T05:36:26.092905: step 2440, loss 0.786542, acc 0.696\n",
      "2016-12-14T05:37:13.578949: step 2441, loss 0.743956, acc 0.73\n",
      "2016-12-14T05:37:58.746390: step 2442, loss 0.792041, acc 0.688\n",
      "2016-12-14T05:38:43.927905: step 2443, loss 0.673044, acc 0.706\n",
      "2016-12-14T05:39:30.412131: step 2444, loss 0.750525, acc 0.684\n",
      "2016-12-14T05:40:15.833874: step 2445, loss 0.807214, acc 0.652\n",
      "2016-12-14T05:41:00.674489: step 2446, loss 0.911573, acc 0.602\n",
      "2016-12-14T05:41:44.813456: step 2447, loss 0.839391, acc 0.666\n",
      "2016-12-14T05:42:31.129065: step 2448, loss 0.638916, acc 0.742\n",
      "2016-12-14T05:43:23.661821: step 2449, loss 0.991721, acc 0.562\n",
      "2016-12-14T05:44:10.987379: step 2450, loss 0.852316, acc 0.648\n",
      "2016-12-14T05:45:00.606132: step 2451, loss 0.644625, acc 0.76\n",
      "2016-12-14T05:45:52.938027: step 2452, loss 0.590341, acc 0.742\n",
      "2016-12-14T05:46:43.517119: step 2453, loss 1.02309, acc 0.552\n",
      "2016-12-14T05:47:29.821372: step 2454, loss 0.9509, acc 0.584\n",
      "2016-12-14T05:48:15.044859: step 2455, loss 0.854597, acc 0.64\n",
      "2016-12-14T05:49:00.918310: step 2456, loss 0.844577, acc 0.642\n",
      "2016-12-14T05:49:47.651647: step 2457, loss 0.919842, acc 0.598\n",
      "2016-12-14T05:50:31.708152: step 2458, loss 0.825985, acc 0.656\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-dd38bfadaf67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mevaluate_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-dd38bfadaf67>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x_batch, y_batch)\u001b[0m\n\u001b[0;32m     79\u001b[0m             _, step, summaries, loss, accuracy = sess.run(\n\u001b[0;32m     80\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 feed_dict)\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 382\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    383\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 655\u001b[1;33m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 723\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    724\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    728\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#sets values for the default graph\n",
    "#defined here to enable easier updating/modifications\n",
    "dropout_keep_prob = 0.5\n",
    "batch_size = 500  #64, 8192\n",
    "num_epochs = 1 #200\n",
    "evaluate_every = 400 #200\n",
    "checkpoint_every = 400 #200\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_review_length)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=max_review_length,\n",
    "            num_classes=5,\n",
    "            vocab_size=len(vocab_index)+1,\n",
    "            embedding_size=128,\n",
    "            filter_sizes=[3,4,5],\n",
    "            num_filters=128)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        #curretnly using the mini set and dev set\n",
    "        batches = batch_iter(list(zip(train_data, train_labels)), batch_size, num_epochs)\n",
    "        #dev_batches = batch_iter(list(zip(dev_data, dev_labels)), 200, 1)\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                clear_output()\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_batches = batch_iter(list(zip(dev_data, dev_labels)), batch_size, 1)\n",
    "                for dev_batch in dev_batches:\n",
    "                    dev_x_batch, dev_y_batch = zip(*dev_batch)\n",
    "                    dev_step(dev_x_batch, dev_y_batch, writer=dev_summary_writer)\n",
    "                #dev_step(dev_data, dev_labels, writer=dev_summary_writer)\n",
    "                clear_output()\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_text = text_process(test_text)\n",
    "test_labels = label_process(test_labels)\n",
    "\n",
    "test_data = [[vocab_index.get(w) for w in r.split(' ')] for r in test_text]\n",
    "test_data = [[w for w in r if w != None] for r in test_data]\n",
    "test_data = data_expand(test_data, max_review_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir=\"runs/1481516781/checkpoints/\"\n",
    "checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "batch_size = 1000\n",
    "graph = tf.Graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

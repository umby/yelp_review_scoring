{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from IPython.display import clear_output\n",
    "from itertools import chain\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to clean the data\n",
    "def clean_string(string):\n",
    "    string = string.lower()\n",
    "    # remove web addresses\n",
    "    string = re.sub(r\"(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \n",
    "                    \" \", string)\n",
    "    string = re.sub(r\"[^a-z0-9!\\?\\']\", \" \", string)\n",
    "    string = re.sub(r\"'s\", \" 's\", string)\n",
    "    string = re.sub(r\"s' \", \" 's\", string)\n",
    "    string = re.sub(r\"'ve\", \" 've\", string)\n",
    "    string = re.sub(r\"n't\", \" n't\", string)\n",
    "    string = re.sub(r\"'re\", \" 're\", string)\n",
    "    string = re.sub(r\"'d\", \" 'd\", string)\n",
    "    string = re.sub(r\"'ll\", \" 'll\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean up the text to only use ascii characters\n",
    "def clean_file(file_path, print_every=20000, write_out=True):\n",
    "    \"\"\"\n",
    "    Read through the CSV file and generate a dictionary of words\n",
    "    with counts and [optional] create a new cleaned file.\n",
    "    \n",
    "    Returns word_count dictionary, and cleaned file path\n",
    "    \"\"\"\n",
    "    word_count = defaultdict(int)\n",
    "    start = time.time()\n",
    "    T0 = time.time()\n",
    "    if write_out:\n",
    "        p, e = os.path.splitext(file_path)\n",
    "        out_path = p + '_clean' + e\n",
    "        fp_out = open(out_path, 'w', encoding='utf-8')\n",
    "        fp_out.write('review,stars\\n')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as fp:\n",
    "        reader = csv.reader(fp)\n",
    "        header = next(reader)\n",
    "        text_index = header.index('text')\n",
    "        star_index = header.index('stars')\n",
    "\n",
    "        for i,line in enumerate(reader):\n",
    "            review_text = clean_string(line[text_index])\n",
    "            stars = line[star_index]\n",
    "            if write_out and len(review_text)>0:\n",
    "                fp_out.write(review_text+','+stars+'\\n')\n",
    "            review = review_text.split(' ')\n",
    "            for word in review:\n",
    "                word_count[word] += 1\n",
    "            if i%print_every == 0:\n",
    "                if write_out:\n",
    "                    fp_out.flush()\n",
    "                print('{} reviews completed, dictionary size: {}, cycle time: {:.1f} seconds.'.format(\n",
    "                    i, len(word_count), time.time()-T0))\n",
    "                T0 = time.time()\n",
    "    print('{} reviews completed.  Dictionary size: {}'.format(i, len(word_count)))\n",
    "    print('Total time: {}'.format(str(timedelta(seconds=round(time.time()-start)))))\n",
    "    if write_out:\n",
    "        fp_out.close()\n",
    "    return word_count, out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wordcount(cleaned_file_path):\n",
    "    \"\"\"\n",
    "    Generates a dictionary of {word, count} from a file that has already\n",
    "    been cleaned.\n",
    "    \"\"\"\n",
    "    word_count = defaultdict(int)\n",
    "    with open(cleaned_file_path, 'r', encoding='utf-8') as fp:\n",
    "        header = next(fp)\n",
    "        for i,line in enumerate(fp):\n",
    "            review_text, stars = line.split(',')\n",
    "            for word in review_text.split(' '):\n",
    "                word_count[word] += 1\n",
    "            if i%100000 == 0:\n",
    "                print('Words in {} reviews counted.'.format(i))\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_vocab(word_count, remove_lowest_fraction=0.01):\n",
    "    \"\"\"\n",
    "    Creates a vocabulary by removing the bottom `remove_lowest_fraction` \n",
    "    of the words passed in via `word_count`.\n",
    "    : inputs : \n",
    "    word_count: a diction of {word, int} pairs\n",
    "    remove_lowest_fraction: the loweset occuring fraction out of 1.0 to\n",
    "        drop from the vocab\n",
    "\n",
    "    : output :\n",
    "    vocab : a dictionary of {word, index} to to use for mapping words to ints\n",
    "    \"\"\"\n",
    "    total_words = sum(list(word_count.values()))\n",
    "    threshold = 0\n",
    "    sum_below_threshold = 0\n",
    "    while float(sum_below_threshold) / total_words < remove_lowest_fraction:\n",
    "        threshold += 1\n",
    "        sum_below_threshold = sum([v for v in word_count.values() if v<=threshold])\n",
    "\n",
    "    print('Word frequency threshold = {} at {:.3f}% percent of total words.'.format(threshold,\n",
    "        100*sum_below_threshold/total_words))\n",
    "    print('Vocab size at 100% = {}, vocab size at {:.3f}% = {}'.format(\n",
    "        len(word_count), \n",
    "        100*(1-sum_below_threshold/total_words),\n",
    "        len([w for w,v in word_count.items() if v > threshold])))\n",
    "\n",
    "    ## create a list of all word above that threshold\n",
    "    ## create a dictionary that maps all valid words to a number\n",
    "    vocab_list = sorted(word_count, key=word_count.get, reverse=True)  #a list\n",
    "    # include '.' which is not in word_count so that 0 is open to use a filler\n",
    "    vocab = {}\n",
    "    for i,w in enumerate(chain(('.', '_RARE_'), vocab_list)):\n",
    "        if w in ('.', '_RARE_'):\n",
    "            vocab[w] = i\n",
    "        elif word_count.get(w) > threshold:\n",
    "            vocab[w] = i\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_nonvocab_words(cleaned_file_path, vocab):\n",
    "    \"\"\"\n",
    "    Reads through cleaned file and remove words not in `vocab` and \n",
    "    removes reviews with zero remaining words.\n",
    "\n",
    "    Writes a new file to ``CLEAN_FILENAME``+``_vocab_only``.csv\n",
    "\n",
    "    Returns a tuple the number of words in the longest review, the\n",
    "    number of reviews, and cleaned and vocab-only file path.\n",
    "    \"\"\"\n",
    "    p, e = os.path.splitext(cleaned_file_path)\n",
    "    out_path = p + '_vocab_only' + e\n",
    "    longest_review_length = 0\n",
    "    number_reviews = 0\n",
    "    with open(cleaned_file_path,'r') as fp, open(out_path, 'w', encoding='utf-8') as fp_out:\n",
    "        #move header to new file\n",
    "        fp_out.write(next(fp))\n",
    "        for i, line in enumerate(fp):\n",
    "            review_text, stars = line.split(',')\n",
    "            review = [w if w in vocab else '_RARE_' for w in review_text.split(' ')]\n",
    "            if longest_review_length < len(review):\n",
    "                longest_review_length = len(review)\n",
    "            if len(review) > 0:\n",
    "                fp_out.write(' '.join(review)+','+stars)\n",
    "                number_reviews += 1\n",
    "            if i%250000 == 0:\n",
    "                print('{} reviews cleaned and removed non-vocab words.'.format(i))\n",
    "        print('{} reviews cleaned and removed non-vocab words.'.format(i))\n",
    "    return longest_review_length, number_reviews, out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordcount(cleaned_file_path):\n",
    "    \"\"\"\n",
    "    Generates a dictionary of {word, count} from a file that has already\n",
    "    been cleaned.\n",
    "    \"\"\"\n",
    "    longest_review_length = 0\n",
    "    word_count = defaultdict(int)\n",
    "    with open(cleaned_file_path, 'r', encoding='utf-8') as fp:\n",
    "        header = next(fp)\n",
    "        for rev_count, line in enumerate(fp):\n",
    "            review_text, stars = line.split(',')\n",
    "            review = review_text.split(' ')\n",
    "            if longest_review_length < len(review):\n",
    "                longest_review_length = len(review)\n",
    "            for word in review:\n",
    "                word_count[word] += 1\n",
    "            if rev_count%200000 == 0:\n",
    "                print('Words in {} reviews counted.'.format(rev_count))\n",
    "    print('Words in {} reviews counted.'.format(rev_count))\n",
    "    return dict(word_count), rev_count, longest_review_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_to_arrays(file_path, vocab, max_review_length, number_reviews):\n",
    "    \"\"\"\n",
    "    Warning: this will need about 5 GB of RAM.\n",
    "\n",
    "    : inputs :\n",
    "    file_path: [string] the path to cleaned csv with non-vocab words already \n",
    "               removed\n",
    "    vocab: [dictionary] mapping of {word, int} from words to integers\n",
    "    max_review_length: [int] the number of words in the longest review \n",
    "    number_reviews: [int] total number of reviews\n",
    "\n",
    "    : outputs :\n",
    "    r_arr: [np.array] a 2D integer array representation of the reviews. The\n",
    "           space for extra words are padded with zeros\n",
    "    s_arr: [np.array] a 2D integer array of the star ratings.\n",
    "    \"\"\"\n",
    "    r_arr = np.zeros((number_reviews, longest_review), dtype=np.int16)\n",
    "    s_arr = np.zeros((number_reviews, 5), dtype=np.int8)\n",
    "    with open(file_path, 'r') as fp:\n",
    "        header = next(fp)\n",
    "        for i, line in enumerate(fp):\n",
    "            review, stars = line.split(',')\n",
    "            rev = [vocab[w] for w in review.split(' ')] \n",
    "            stars = int(stars)\n",
    "            r_arr[i,:len(rev)] = rev\n",
    "            s_arr[i,stars-1] = 1\n",
    "    return r_arr, s_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc, num_rev, seq_len = get_wordcount('/share/yelp/yelp_academic_dataset_review_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = create_vocab(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/share/yelp/params_voc_seq_num.pickle', 'wb') as fp:\n",
    "    pickle.dump((vocab, seq_len, num_rev), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq, num, p = remove_nonvocab_words('/share/yelp/yelp_academic_dataset_review_clean.csv', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \n",
    "    Baseline model based off of example from WildML (link below)\n",
    "    http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FileBatcher(object):\n",
    "    \"\"\"\n",
    "    An object to create batches of data from a file. \n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    sequence_length = None\n",
    "    num_items = None\n",
    "    split_line = None\n",
    "    \n",
    "    def __init__(self, file_path, split_ratio):\n",
    "        \"\"\"\n",
    "        Opens a file for reading using a vocabulary to translate the words\n",
    "        to numbers.  The test ratio is the fraction of data to withhold\n",
    "        for evaluation.\n",
    "        \"\"\"                \n",
    "        self.file_path = file_path\n",
    "        self.split_ratio = split_ratio\n",
    "    \n",
    "    def load_params(self, vocab, sequence_length, num_items):\n",
    "        \"\"\"\n",
    "        Input the vocabulary, maximum sequency length, and total number\n",
    "        of item in the file.\n",
    "        \"\"\"\n",
    "        assert isinstance(vocab, dict), '`vocab` must be a dictionary.'\n",
    "        self.vocab = vocab\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_items = num_items\n",
    "        self.split_line = int(num_items*self.split_ratio)\n",
    "        \n",
    "    def get_params(self, verbose=True):\n",
    "        \"\"\"\n",
    "        Reads the file at `file_path` and sets the .vocab,\n",
    "        .sequence_length, and .num_items paramters.\n",
    "        \"\"\"\n",
    "        from itertools import chain\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        SL = 0\n",
    "        v = defaultdict(int)\n",
    "        with open(self.file_path) as fp:\n",
    "            for count, line in enumerate(fp):\n",
    "                rev_txt, star = line.strip().split(',')\n",
    "                rev = rev_txt.split()\n",
    "                for w in rev:\n",
    "                    v[w] += 1\n",
    "                SL = len(rev) if len(rev)>SL else SL\n",
    "                if verbose:\n",
    "                    if count%100000 == 0:\n",
    "                        print('Total lines processed: {:,}, vocab size: {}'\n",
    "                               .format(count, len(v)))\n",
    "        self.vocab = dict((w,i) for i,w in \n",
    "            enumerate(chain(' ',sorted(v, key=v.get, reverse=True))))\n",
    "        self.sequence_length = SL\n",
    "        self.num_items = count\n",
    "        self.split_line = int(count*self.split_ratio)\n",
    "    \n",
    "    def _line_parser(self, line):\n",
    "        \"\"\"\n",
    "        Uses .vocab to convert a list of words to integers padded\n",
    "        out to .sequence_length.  \n",
    "        \"\"\"\n",
    "        # split the line\n",
    "        rev_txt, stars = line.strip().split(',')\n",
    "        # convert stars to wide format\n",
    "        stars = int(stars)\n",
    "        stars_wide = [0]*(stars-1)+[1]+[0]*(5-stars)\n",
    "        # convert words to ints\n",
    "        rev = [self.vocab[w] for w in rev_txt.split(' ')]\n",
    "        # pad the review\n",
    "        rev_wide = rev + [0]*(self.sequence_length-len(rev))\n",
    "        # return combined\n",
    "        return rev_wide + stars_wide\n",
    "\n",
    "    # creates batches of the data so everything is not stored in memory at once\n",
    "    def batch_iter(self, batch_size, num_epochs, batch_pool_size=16000,\n",
    "                   shuffle=True, split='none'):\n",
    "        \"\"\"\n",
    "        Generates a batch iterator for a dataset from a file_path\n",
    "\n",
    "        :inputs:\n",
    "        batch_size - int, number of items per batch\n",
    "        num_epochs - int, number of epochs to iterate over the data\n",
    "        batch_pool_size - int, number of lines of data to use for pooling \n",
    "                          when creating the iterator\n",
    "        shuffle - bool, whether to shuffle the pool before batching\n",
    "        split - string {`first`|`last`|`none`}, \n",
    "        ::-- `first` uses up to the first `.split_ratio` fraction of the file lines.  \n",
    "        ::-- `last` uses the lines beyond the first`.split_ratio`\n",
    "        ::-- `none` uses the entire file\n",
    "        \"\"\"\n",
    "        assert isinstance(split, str), 'Paramter `split` must be a string.'\n",
    "        if split.lower()=='none':\n",
    "            iter_size = self.num_items\n",
    "        elif split.lower()=='first':\n",
    "            iter_size = self.split_line\n",
    "        elif split.lower()=='last':\n",
    "            iter_size = self.num_items - self.split_line\n",
    "        else:\n",
    "            raise ValueError('Value of `split` must be in {\"first\",\"last\",\"none\"}')\n",
    "            \n",
    "        # get the number of batches and pool size of the leftovers\n",
    "        number_pools, last_pool_size = divmod(iter_size, batch_pool_size)\n",
    "        # combine the leftovers if it is less than 1/4 of the pool size\n",
    "        if number_pools > 0 and last_pool_size < batch_pool_size/4:\n",
    "            last_pool_size += batch_pool_size\n",
    "        # add a batch pool for the left overs\n",
    "        else:\n",
    "            number_pools += 1\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            with open(self.file_path) as fp:\n",
    "                # remove header lines\n",
    "                header = next(fp)\n",
    "                # move the file point to the correct starting location\n",
    "                if split.lower()=='last':\n",
    "                    for i in range(self.split_line):\n",
    "                        null = next(fp)\n",
    "                # pull in the next pool of batches\n",
    "                for i in range(1,number_pools+1):\n",
    "                    p_size = batch_pool_size if i<number_pools else last_pool_size\n",
    "                    batch_pool = np.array([self._line_parser(next(fp)) \n",
    "                                           for j in range(p_size)], dtype=int)\n",
    "\n",
    "                    # Shuffle the data at each epoch\n",
    "                    if shuffle:\n",
    "                        shuffle_indices = np.random.permutation(np.arange(p_size))\n",
    "                        batch_pool = batch_pool[shuffle_indices]\n",
    "                    \n",
    "                    # set the number of iterations for each pool size\n",
    "                    number_batches = int(np.ceil(p_size/float(batch_size)))\n",
    "                    for batch_num in range(number_batches):\n",
    "                        start_index = batch_num * batch_size\n",
    "                        end_index = min((batch_num + 1) * batch_size, p_size)\n",
    "                        \n",
    "                        yield (batch_pool[start_index:end_index,:-5], \n",
    "                               batch_pool[start_index:end_index,-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-13T18:30:31.252325: step 43951, loss 0.696623, acc 0.75\n",
      "2016-12-13T18:30:32.797298: step 43952, loss 0.780235, acc 0.65625\n",
      "2016-12-13T18:30:34.364364: step 43953, loss 0.775759, acc 0.65625\n",
      "2016-12-13T18:30:36.051557: step 43954, loss 0.486717, acc 0.828125\n",
      "2016-12-13T18:30:37.567994: step 43955, loss 0.77579, acc 0.671875\n",
      "2016-12-13T18:30:39.195823: step 43956, loss 0.792433, acc 0.640625\n",
      "2016-12-13T18:30:40.825692: step 43957, loss 0.747814, acc 0.671875\n",
      "2016-12-13T18:30:42.479582: step 43958, loss 0.740036, acc 0.75\n",
      "2016-12-13T18:30:44.066846: step 43959, loss 0.670348, acc 0.71875\n"
     ]
    }
   ],
   "source": [
    "#sets values for the default graph\n",
    "#defined here to enable easier updating/modifications\n",
    "dropout_keep_prob = 0.5\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "evaluate_every = 2000\n",
    "checkpoint_every = 20000\n",
    "dev_fraction = 0.001\n",
    "\n",
    "# create batching object\n",
    "with open('/share/yelp/params_voc_seq_num.pickle', 'rb') as fp:\n",
    "    vocab, seq_len, num_lines = pickle.load(fp)\n",
    "file_path = '/share/yelp/yelp_academic_dataset_review_clean_vocab_only.csv'\n",
    "fb = FileBatcher(file_path, dev_fraction)\n",
    "fb.load_params(vocab, seq_len, num_lines)\n",
    "train_batches = fb.batch_iter(batch_size, num_epochs, split='last')\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=seq_len,\n",
    "            num_classes=5,\n",
    "            vocab_size=len(vocab),\n",
    "            embedding_size=32,\n",
    "            filter_sizes=[3,4,5],\n",
    "            num_filters=128)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for x_batch, y_batch in train_batches:\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step%50==0:\n",
    "                clear_output()\n",
    "            if (current_step > 0) and (current_step % evaluate_every == 0):\n",
    "                dev_batches = fb.batch_iter(batch_size, num_epochs=1, shuffle=False, split='first')\n",
    "                clear_output()\n",
    "                print(\"\\nEvaluation:\")\n",
    "                for d_i, (x_batch, y_batch) in enumerate(dev_batches):\n",
    "                    dev_step(x_batch, y_batch, writer=dev_summary_writer)\n",
    "                    if d_i%20==0:\n",
    "                        clear_output()\n",
    "                        print(\"\\nEvaluation - cycle: {}\".format(d_i))\n",
    "                clear_output()\n",
    "            if (current_step > 0) and (current_step % checkpoint_every == 0):\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sets values for the default graph\n",
    "#defined here to enable easier updating/modifications\n",
    "dropout_keep_prob = 0.5\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "evaluate_every = 2000\n",
    "checkpoint_every = 20000\n",
    "dev_fraction = 0.001\n",
    "\n",
    "# read in params again\n",
    "with open('/share/yelp/params_voc_seq_num.pickle', 'rb') as fp:\n",
    "    vocab, seq_len, num_lines = pickle.load(fp)\n",
    "\n",
    "# load up a very similar graph for evaluations\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=seq_len,\n",
    "            num_classes=5,\n",
    "            vocab_size=len(vocab),\n",
    "            embedding_size=32,\n",
    "            filter_sizes=[3,4,5],\n",
    "            num_filters=128)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "\n",
    "        saver = tf.train.Saver(tf.all_variables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
